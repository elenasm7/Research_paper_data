{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Preprocessing, and Feature Engineering\n",
    "\n",
    "In this notebook, we will read in our data. Match instances in our data to images, and add the file path to the df.\n",
    "After, joining our data, we will need to cross join to get Irradiance data with it's iterval ahead weather, irradiance, and sky images. \n",
    "\n",
    "All data was dowloaded from __[here](https://zenodo.org/record/2826939#.YEPKXi1h1pS)__. Thanks so much to the University of California San Diego team (Carreira Pedro, Hugo; Larson, David; Coimbra, Carlos) who worked so hard on collecting this data, and for supporting the work of others in this space.\n",
    "\n",
    "#### Below we will:\n",
    "1. [Create dataframes](#Create-filepaths-to-images)\n",
    "    - create filepaths for images\n",
    "    - get time intervals and cross join for earlier irradiance data\n",
    "    - merge weather and irradiance data\n",
    "    \n",
    "    \n",
    "2. [join our data & get data at intervals before](#)\n",
    "3. [explore our data](#)\n",
    "4. [preprocess/scale our data](#)\n",
    "5. [Feature engineering](#)\n",
    "\n",
    "\n",
    "Import needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bz2\n",
    "from datetime import datetime,timedelta\n",
    "import tarfile\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions needed for all processes in notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakdown_dates(df,column): \n",
    "    df['year'] = pd.DatetimeIndex(df[column]).year\n",
    "    df['month'] = pd.DatetimeIndex(df[column]).month\n",
    "    df['day'] = pd.DatetimeIndex(df[column]).day\n",
    "    df['hour'] = pd.DatetimeIndex(df[column]).hour\n",
    "    df['min'] = pd.DatetimeIndex(df[column]).minute\n",
    "    df['sec'] = pd.DatetimeIndex(df[column]).second\n",
    "    return\n",
    "\n",
    "def datetime_blank_min_before(df,dt):\n",
    "    df[f'{dt}_min_before'] = pd.DatetimeIndex(df['timestamp']) - timedelta(minutes=dt)\n",
    "    return \n",
    "\n",
    "def numberOfDays(y, m):\n",
    "    leap = 0\n",
    "    if y% 400 == 0:\n",
    "        leap = 1\n",
    "    elif y % 100 == 0:\n",
    "        leap = 0\n",
    "    elif y% 4 == 0:\n",
    "        leap = 1\n",
    "    if m==2:\n",
    "        return 28 + leap\n",
    "    list = [1,3,5,7,8,10,12]\n",
    "    if m in list:\n",
    "        return 31\n",
    "    return 30\n",
    "\n",
    "def make_image_path(row):\n",
    "    files = {}\n",
    "    files['higher_file_path'] = make_higher_image_path(row)\n",
    "    files['lower_file_path'] = make_lower_image_path(row)\n",
    "    return files\n",
    "\n",
    "def get_all_file_names(li_file_names,file_name_dict):\n",
    "    for i in li_file_names:\n",
    "        if '.DS_Store' in i: \n",
    "            pass\n",
    "        elif len(i) > 4 and len(i) <= 7:\n",
    "            yrmn = i \n",
    "            file_name_dict[i] = {}\n",
    "        elif len(i) > 7 and len(i) <= 10:\n",
    "            yrmnd = i\n",
    "            file_name_dict[yrmn][i] = []\n",
    "        elif len(i) > 10:\n",
    "            try:\n",
    "                file_name_dict[yrmn][yrmnd].append(i)\n",
    "            except:\n",
    "                print(i)\n",
    "                break\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "# 2014/12/29/20141229_170300.jpg\n",
    "def make_higher_image_path(row):\n",
    "    mn,day,hour,mi,yr = (int(row[\"month\"]),int(row[\"day\"]),\n",
    "                      int(row[\"hour\"]),int(row[\"min\"]),int(row[\"year\"]))\n",
    "    \n",
    "    return datetime(yr,mn,day,hour,mi,46)\n",
    "\n",
    "def make_lower_image_path(row):\n",
    "    mn,day,hour,mi,yr = (row[\"month\"],row[\"day\"],\n",
    "                      row[\"hour\"],row[\"min\"],row[\"year\"])\n",
    "    mn = f\"{mn:02}\"\n",
    "    day = f\"{day:02}\"\n",
    "    hour = f\"{hour:02}\"\n",
    "    mi = f\"{mi:02}\"\n",
    "    yr = str(yr)\n",
    "    \n",
    "    if mi == \"00\":\n",
    "        if hour == \"00\":\n",
    "            if (day == \"01\") and (mn == \"01\"):\n",
    "                y = int(yr) - 1 \n",
    "                if y in [2014,2015,2016]:\n",
    "                    yr_l = y\n",
    "                    mn_l = 12\n",
    "                    day_l = numberOfDays(y, mn_l)\n",
    "                    hour_1 = 23\n",
    "                    mi_l = 59\n",
    "                    s_l = 45\n",
    "                else:\n",
    "                    yr_1 = int(yr)\n",
    "                    mn_l = int(mn)\n",
    "                    hour_1 = int(hour)\n",
    "                    day_l = int(day)\n",
    "                    mi_l = 0\n",
    "                    s_l = 0\n",
    "\n",
    "            elif (day == \"01\") and (mn != \"01\"):\n",
    "                yr_l = int(yr)\n",
    "                mn_l = int(mn) - 1\n",
    "                day_l = numberOfDays(int(yr), mn_l)\n",
    "                hour_1 = 23\n",
    "                mi_l = 59\n",
    "                s_l = 45\n",
    "            else:\n",
    "                yr_l = int(yr)\n",
    "                mn_l = int(mn)\n",
    "                day_l = int(day) - 1\n",
    "                hour_1 = 23\n",
    "                mi_l = 59\n",
    "                s_l = 45\n",
    "        else:\n",
    "            yr_l = int(yr)\n",
    "            day_l = int(day)\n",
    "            mn_l = int(mn)\n",
    "            hour_1 = int(hour) - 1\n",
    "            mi_l = 59\n",
    "            s_l = 45\n",
    "    else:\n",
    "        hour_1 = int(hour)\n",
    "        day_l = int(day)\n",
    "        mn_l = int(mn)\n",
    "        yr_l = int(yr)\n",
    "        mi_l = int(mi) - 1\n",
    "        s_l = 45\n",
    "        \n",
    "    return datetime(yr_l,mn_l,day_l,hour_1,mi_l,s_l)\n",
    "\n",
    "\n",
    "def get_correct_file(row,file_dict):\n",
    "    mn,day,hour,mi = (row[\"month\"],row[\"day\"],\n",
    "                      row[\"hour\"],row[\"min\"])\n",
    "    higher, lower = row[\"higher_file\"],row[\"lower_file\"]\n",
    "    \n",
    "    mn = f\"{mn:02}\"\n",
    "    day = f\"{day:02}\"\n",
    "    hour = f\"{hour:02}\"\n",
    "    mi = f\"{mi:02}\"\n",
    "    \n",
    "    yrmn,date = f\"{str(row['year'])}/{mn}\", f\"{str(row['year'])}/{mn}/{day}\"\n",
    "    if date in file_dict[str(row['year'])][yrmn].keys():\n",
    "        for file in file_dict[str(row['year'])][yrmn][date]:\n",
    "            dt_f = datetime(int(file[:4]),int(file[5:7]),int(file[8:10]),int(file[20:22]),int(file[22:24]),int(file[24:26]))\n",
    "            \n",
    "            if (dt_f >= lower) and (dt_f <= higher):\n",
    "                return file\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def save_pickle(file_name,obj):\n",
    "    with open(file_name, 'wb') as fout:\n",
    "        pickle.dump(obj, fout)\n",
    "\n",
    "def open_pickle(file_name):\n",
    "    with open(file_name, 'rb') as handle:\n",
    "        obj = pickle.load(handle)\n",
    "    return obj\n",
    "\n",
    "def update_df_for_model(df,column):\n",
    "    col = column + '_i'\n",
    "#     df['Y'] = df.apply(lambda row: [row['ghi_x'],row['dni_x'],row['dhi_x']],axis=1)\n",
    "#     df[col] = df.apply(lambda row: [row['ghi_y'],row['dni_y'],row['dhi_y']],axis=1)\n",
    "    df = df[['ghi_x','timestamp_x',column,'air_temp','relhum', 'press', 'windsp', \n",
    "             'winddir', 'max_windsp', 'precipitation','file','ghi_y']]\n",
    "\n",
    "    return df.rename(columns={'timestamp_x':'timestamp','ghi_x':'Y','ghi_y':col})\n",
    "\n",
    "def preview_df(df):\n",
    "    df_dtypes = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
    "    df_dtypes = df_dtypes.reset_index()\n",
    "    df_dtypes['name'] = df_dtypes['index']\n",
    "    df_dtypes = df_dtypes[['name','dtypes']]\n",
    "    df_dtypes['first value'] = df.loc[0].values\n",
    "    data_dictionary = pd.DataFrame(df.columns).rename(columns={0:\"name\"})\n",
    "    preview = df_dtypes.merge(data_dictionary, on='name',how='left')\n",
    "    \n",
    "    return preview\n",
    "\n",
    "#need to look into how these may have been saved incorrectly, so if they're off then \n",
    "# they can be matched to the closest file by second, maybe there should be an upper \n",
    "# and lower time within the min?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create filepaths to images\n",
    "\n",
    "\n",
    "Open the image files to get image names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img_file_names = {}\n",
    "for yr in [2014,2015,2016]:\n",
    "    f_name = f'data/Folsom_sky_images_{yr}.tar.bz2'\n",
    "    print(f_nmae)\n",
    "    tar = tarfile.open(f_name, \"r\")\n",
    "    tar_members_names = [filename for filename in tar.getnames()]\n",
    "    img_file_names[yr] = {}\n",
    "    get_all_file_names(tar_members_names,img_file_names[yr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle('image_file_names.pkl',new_img_file_names)\n",
    "# save_pickle('data/df_solar_and_img_data.pkl',df_merge_1)\n",
    "img_file_names = open_pickle('data/image_file_names.pkl')\n",
    "df_merge_1 = open_pickle('data/df_solar_and_img_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fol_irr = pd.read_csv('data/Folsom_irradiance.csv',index_col=0)\n",
    "fol_sat = pd.read_csv('data/Folsom_satellite.csv')\n",
    "fol_sky_img = pd.read_csv('data/Folsom_sky_image_features.csv',index_col=0)\n",
    "fol_weather = pd.read_csv('data/Folsom_weather.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fol_sat.columns\n",
    "datetime_blank_min_before(fol_irr,5)\n",
    "datetime_blank_min_before(fol_irr,10)\n",
    "datetime_blank_min_before(fol_irr,15)\n",
    "datetime_blank_min_before(fol_irr,20)\n",
    "datetime_blank_min_before(fol_irr,25)\n",
    "datetime_blank_min_before(fol_irr,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fol_weather['timestamp'] = pd.to_datetime(fol_weather['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_min_ahead = pd.merge(fol_irr,fol_weather,how=\"left\", left_on=\"5_min_before\", right_on=\"timestamp\")\n",
    "df_10_min_ahead = pd.merge(fol_irr,fol_weather,how=\"left\", left_on=\"10_min_before\", right_on=\"timestamp\")\n",
    "df_15_min_ahead = pd.merge(fol_irr,fol_weather,how=\"left\", left_on=\"15_min_before\", right_on=\"timestamp\")\n",
    "df_20_min_ahead = pd.merge(fol_irr,fol_weather,how=\"left\", left_on=\"20_min_before\", right_on=\"timestamp\")\n",
    "df_25_min_ahead = pd.merge(fol_irr,fol_weather,how=\"left\", left_on=\"25_min_before\", right_on=\"timestamp\")\n",
    "df_30_min_ahead = pd.merge(fol_irr,fol_weather,how=\"left\", left_on=\"30_min_before\", right_on=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_min_ahead = df_5_min_ahead.dropna()[['timestamp_x','5_min_before', 'ghi', 'dni', 'dhi',\n",
    "                                  'air_temp', 'relhum', 'press', 'windsp', 'winddir','max_windsp', \n",
    "                                  'precipitation']].rename(columns={'timestamp_x':'timestamp'})\n",
    "\n",
    "df_10_min_ahead = df_10_min_ahead.dropna()[['timestamp_x','10_min_before', 'ghi', 'dni', 'dhi',\n",
    "                                  'air_temp', 'relhum', 'press', 'windsp', 'winddir','max_windsp', \n",
    "                                  'precipitation']].rename(columns={'timestamp_x':'timestamp'})\n",
    "\n",
    "df_15_min_ahead = df_15_min_ahead.dropna()[['timestamp_x','15_min_before', 'ghi', 'dni', 'dhi',\n",
    "                                  'air_temp', 'relhum', 'press', 'windsp', 'winddir','max_windsp', \n",
    "                                  'precipitation']].rename(columns={'timestamp_x':'timestamp'})\n",
    "\n",
    "df_20_min_ahead = df_20_min_ahead.dropna()[['timestamp_x','20_min_before', 'ghi', 'dni', 'dhi',\n",
    "                                  'air_temp', 'relhum', 'press', 'windsp', 'winddir','max_windsp', \n",
    "                                  'precipitation']].rename(columns={'timestamp_x':'timestamp'})\n",
    "\n",
    "df_25_min_ahead = df_25_min_ahead.dropna()[['timestamp_x','25_min_before', 'ghi', 'dni', 'dhi',\n",
    "                                  'air_temp', 'relhum', 'press', 'windsp', 'winddir','max_windsp', \n",
    "                                  'precipitation']].rename(columns={'timestamp_x':'timestamp'})\n",
    "\n",
    "df_30_min_ahead = df_30_min_ahead.dropna()[['timestamp_x','30_min_before', 'ghi', 'dni', 'dhi',\n",
    "                                  'air_temp', 'relhum', 'press', 'windsp', 'winddir','max_windsp', \n",
    "                                  'precipitation']].rename(columns={'timestamp_x':'timestamp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in [df_5_min_ahead,df_10_min_ahead,df_15_min_ahead,df_20_min_ahead,df_25_min_ahead,df_30_min_ahead]:\n",
    "#     print(table.columns[1])\n",
    "    breakdown_dates(table,table.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in [df_5_min_ahead,df_10_min_ahead,df_15_min_ahead,df_20_min_ahead,df_25_min_ahead,df_30_min_ahead]:\n",
    "    print(table.columns[1])\n",
    "    table['higher_file'] = table.apply(lambda row: make_higher_image_path(row),axis=1)\n",
    "    table['lower_file'] = table.apply(lambda row: make_lower_image_path(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in [df_5_min_ahead,df_10_min_ahead,df_15_min_ahead,df_20_min_ahead,df_25_min_ahead,df_30_min_ahead]:\n",
    "    print(table.columns[1])\n",
    "    table['file'] = table.apply(lambda row: get_correct_file(row,img_file_names),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_min_ahead_w_img = df_5_min_ahead[(~df_5_min_ahead.file.isnull()) & (df_5_min_ahead.file != 0)]\n",
    "df_10_min_ahead_w_img = df_10_min_ahead[(~df_10_min_ahead.file.isnull()) & (df_10_min_ahead.file != 0)]\n",
    "df_15_min_ahead_w_img = df_15_min_ahead[(~df_15_min_ahead.file.isnull()) & (df_15_min_ahead.file != 0)]\n",
    "df_20_min_ahead_w_img = df_20_min_ahead[(~df_20_min_ahead.file.isnull()) & (df_20_min_ahead.file != 0)]\n",
    "df_25_min_ahead_w_img = df_25_min_ahead[(~df_25_min_ahead.file.isnull()) & (df_25_min_ahead.file != 0)]\n",
    "df_30_min_ahead_w_img = df_30_min_ahead[(~df_30_min_ahead.file.isnull()) & (df_30_min_ahead.file != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle(\"df_5_min_ahead_data.pkl\",df_5_min_ahead_w_img)\n",
    "# save_pickle(\"df_10_min_ahead_data.pkl\",df_10_min_ahead_w_img)\n",
    "# save_pickle(\"df_15_min_ahead_data.pkl\",df_15_min_ahead_w_img)\n",
    "# save_pickle(\"df_20_min_ahead_data.pkl\",df_20_min_ahead_w_img)\n",
    "# save_pickle(\"df_25_min_ahead_data.pkl\",df_25_min_ahead_w_img)\n",
    "# save_pickle(\"df_30_min_ahead_data.pkl\",df_30_min_ahead_w_img)\n",
    "df_5_min_ahead_w_img = open_pickle(\"df_5_min_ahead_data.pkl\")\n",
    "df_10_min_ahead_w_img = open_pickle(\"df_10_min_ahead_data.pkl\")\n",
    "df_15_min_ahead_w_img = open_pickle(\"df_15_min_ahead_data.pkl\")\n",
    "df_20_min_ahead_w_img = open_pickle(\"df_20_min_ahead_data.pkl\")\n",
    "df_25_min_ahead_w_img = open_pickle(\"df_25_min_ahead_data.pkl\")\n",
    "df_30_min_ahead_w_img = open_pickle(\"df_30_min_ahead_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_min_ahead_w_img = df_5_min_ahead_w_img[['timestamp', '5_min_before', 'ghi', 'air_temp', \n",
    "                                             'relhum','press', 'windsp', 'winddir', \n",
    "                                             'max_windsp', 'precipitation', 'file']]\n",
    "df_10_min_ahead_w_img = df_10_min_ahead_w_img[['timestamp', '10_min_before', 'ghi','air_temp', \n",
    "                                               'relhum','press', 'windsp', 'winddir', \n",
    "                                               'max_windsp', 'precipitation', 'file']]\n",
    "df_15_min_ahead_w_img = df_15_min_ahead_w_img[['timestamp', '15_min_before', 'ghi', \n",
    "                                               'air_temp', 'relhum','press', 'windsp', 'winddir', \n",
    "                                               'max_windsp', 'precipitation', 'file']]\n",
    "df_20_min_ahead_w_img = df_20_min_ahead_w_img[['timestamp', '20_min_before', 'ghi', \n",
    "                                               'air_temp', 'relhum','press', 'windsp', 'winddir', \n",
    "                                               'max_windsp', 'precipitation', 'file']]\n",
    "df_25_min_ahead_w_img = df_25_min_ahead_w_img[['timestamp', '25_min_before', 'ghi', \n",
    "                                               'air_temp', 'relhum','press', 'windsp', 'winddir', \n",
    "                                               'max_windsp', 'precipitation', 'file']]\n",
    "df_30_min_ahead_w_img = df_30_min_ahead_w_img[['timestamp', '30_min_before', 'ghi', \n",
    "                                               'air_temp', 'relhum','press', 'windsp', 'winddir', \n",
    "                                               'max_windsp', 'precipitation', 'file']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update datetime format on all DFs before we join to get irradiance from earlier timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_5_min_ahead_w_img,df_10_min_ahead_w_img,df_15_min_ahead_w_img,\n",
    "           df_20_min_ahead_w_img,df_25_min_ahead_w_img,df_30_min_ahead_w_img]:\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fol_irr['timestamp'] = pd.to_datetime(fol_irr['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join tables, to match on their time ahead intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_min = df_5_min_ahead_w_img.merge(fol_irr,how=\"left\", left_on=\"5_min_before\", right_on=\"timestamp\")\n",
    "df_10_min = df_10_min_ahead_w_img.merge(fol_irr,how=\"left\", left_on=\"10_min_before\", right_on=\"timestamp\")\n",
    "df_15_min = df_15_min_ahead_w_img.merge(fol_irr,how=\"left\", left_on=\"15_min_before\", right_on=\"timestamp\")\n",
    "df_20_min = df_20_min_ahead_w_img.merge(fol_irr,how=\"left\", left_on=\"20_min_before\", right_on=\"timestamp\")\n",
    "df_25_min = df_25_min_ahead_w_img.merge(fol_irr,how=\"left\", left_on=\"25_min_before\", right_on=\"timestamp\")\n",
    "df_30_min = df_30_min_ahead_w_img.merge(fol_irr,how=\"left\", left_on=\"30_min_before\", right_on=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_20_min.head(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_min = update_df_for_model(df_5_min,\"5_min_before\")\n",
    "df_10_min = update_df_for_model(df_10_min,\"10_min_before\")\n",
    "df_15_min = update_df_for_model(df_15_min,\"15_min_before\")\n",
    "df_20_min = update_df_for_model(df_20_min,\"20_min_before\")\n",
    "df_25_min = update_df_for_model(df_25_min,\"25_min_before\")\n",
    "df_30_min = update_df_for_model(df_30_min,\"30_min_before\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_5_min.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering & Pre-Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_5_min,df_10_min,df_15_min,df_20_min,df_25_min,df_30_min]:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previews = []\n",
    "for df in [df_5_min,df_10_min,df_15_min,df_20_min,df_25_min,df_30_min]:\n",
    "    previews.append(preview_df(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in img_file_names['2014']['2014/01']:\n",
    "#     print(i,i.find('20140101'))\n",
    "# img_file_names['2014']['2014/01']['2014/01/02']\n",
    "\n",
    "# img_file_names['2014']['2014/01']['2014/01/02'][0][22:26]\n",
    "# img_file_names['2014']['2014/01']['2014/01/02']\n",
    "\n",
    "# tester_1 = df_merge_1.iloc[448:500]\n",
    "# tester_1['file'] = tester_1.apply(lambda row: get_correct_file(row,img_file_names),axis=1)\n",
    "# tester_1[['timestamp','file']]\n",
    "# tester_1[tester_1.file.isnull()]['files'].iloc[0]\n",
    "\n",
    "# df_merge_1.iloc[448:500].apply(lambda row: get_correct_file(row,img_file_names),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merge_1_w_imgs[df_merge_1_w_imgs['file'] == 0]\n",
    "# 18720 - 16848\n",
    "# c - df_merge_1_w_imgs.shape[0]\n",
    "# 16848\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c - 764855"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check to see which image files are not being captured by the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped = {}\n",
    "\n",
    "for yr in img_file_names.keys():\n",
    "    skipped[yr] = {}\n",
    "    for yrmn in img_file_names[yr].keys():\n",
    "        skipped[yr][yrmn] = {}\n",
    "        for yrmnday in img_file_names[yr][yrmn].keys():\n",
    "            skipped[yr][yrmn][yrmnday] = []\n",
    "            for time in img_file_names[yr][yrmn][yrmnday]:\n",
    "                if df_merge_1_w_imgs.file.isin([time]).sum() == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    skipped[yr][yrmn][yrmnday].append(time)\n",
    "#             c += len(img_file_names[yr][yrmn][yrmnday])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the seconds that the fall within:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_skipped = {}\n",
    "\n",
    "for yr in skipped.keys():\n",
    "    for yrmn in skipped[yr].keys():\n",
    "        for yrmnday in skipped[yr][yrmn].keys():\n",
    "            for time in skipped[yr][yrmn][yrmnday]:\n",
    "                s = time[24:26]\n",
    "                if s in seconds_skipped.keys():\n",
    "                    seconds_skipped[s].append(time)\n",
    "                else:\n",
    "                    seconds_skipped[s] = []\n",
    "                    seconds_skipped[s].append(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the hours that the fall within:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_skipped = {}\n",
    "\n",
    "for yr in skipped.keys():\n",
    "    for yrmn in skipped[yr].keys():\n",
    "        for yrmnday in skipped[yr][yrmn].keys():\n",
    "            for time in skipped[yr][yrmn][yrmnday]:\n",
    "                s = time[20:22]\n",
    "                if s in hours_skipped.keys():\n",
    "                    hours_skipped[s].append(time)\n",
    "                else:\n",
    "                    hours_skipped[s] = []\n",
    "                    hours_skipped[s].append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get counts for the seconds below:\n",
    "# seconds_skipped.keys()\n",
    "\n",
    "# for i in ['42','41', '40', '44', '47', '46', '51', '50', '54', '55','59','56','58','53','57']:\n",
    "#     print(i,len(seconds_skipped[i]))\n",
    "    \n",
    "#look into the groups, figure out why it's not being caught:\n",
    "# seconds_skipped['42']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped['2014']['2014/01'].keys()\n",
    "# skipped['2014']['2014/01']['2014/01/02']\n",
    "# '2014/01/02/20140102_004912.jpg'[24:26]\n",
    "# df_merge_1[(df_merge_1['hour'] ==0)&(df_merge_1['day'] == 12)&(df_merge_1['year'] == 2014)&(df_merge_1['month'] == 11)][['timestamp','files','file']]\n",
    "df_merge_1[(df_merge_1['hour'] ==0)&(df_merge_1['day'] == 12)&(df_merge_1['year'] == 2014)&(df_merge_1['month'] == 11)]['files'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c\n",
    "# df_merge_1[(df_merge_1['hour'] ==19)&(df_merge_1['day'] == 14)&(df_merge_1['year'] == 2015)&(df_merge_1['month'] == 12)][['files']].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_1_w_imgs[['timestamp','file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_1_w_imgs[['timestamp','file']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "765159\n",
    "vs.\n",
    "279863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_test_2 = pd.merge(fol_irr, fol_sky_img,\n",
    "                        how=\"left\", on=\"timestamp\")\n",
    "merge_test_2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_test = pd.merge(fol_irr, fol_sky_img,\n",
    "                        how=\"left\", on=[\"year\",\"month\",\"day\",\"hour\",\"min\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fol_irr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fol_sat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fol_sky_img.rename(columns={'timestamp':'timeStamp'},inplace=True)\n",
    "# fol_sky_img.head()\n",
    "# fol_sky_img[fol_sky_img['timeStamp']=='2014-01-02 08:00:00']\n",
    "# fol_waether.iloc[0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_paper",
   "language": "python",
   "name": "research_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
